# Final_Maths_DP_Project



VISHAL SINGH THAKUR 

JUAN SEBASTIAN SIERRA ARIZA 

# Fine-Tuning LSTM and BERT Transformer Models for COVID-19 Tweets - Text Classification

## Introduction
The aim of this project is to fine-tune LSTM and BERT Transformer models for text classification using a dataset related to COVID-19. The project will focus on improving the performance of these models specifically for the COVID-19 text classification task.

## Dataset
The project will utilize the [COVID-19 NLP Text Classification dataset](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv) available on Kaggle. This dataset contains text samples related to COVID-19, along with their corresponding class labels, such as sentiment, emotion, or other relevant categories.

## Scope and Objectives
The project's scope includes the following objectives:

### Preprocessing and Data Analysis
- Perform exploratory data analysis to gain insights into the dataset.
- Preprocess the text data by removing noise, tokenizing, and normalizing the text.
- Handle data cleaning and missing values, if any.
- Perform feature engineering, if required, to extract meaningful features from the text.

### Model Selection and Architecture
- Select an LSTM model and a BERT Transformer model for text classification.
- Utilize pre-trained models for LSTM and BERT as the starting point.
- Fine-tune both models on the COVID-19 dataset by updating the model parameters using backpropagation.
- Define the necessary layers and modifications in the models to adapt them to the COVID-19 text classification task.

### Fine-Tuning and Evaluation
- Fine-tune the LSTM and BERT models using the preprocessed dataset.
- Apply techniques such as gradual unfreezing and learning rate schedules during fine-tuning for better results.
- Evaluate the performance of both models using appropriate evaluation metrics such as accuracy, precision, recall, and F1-score.
- Compare the performance of the fine-tuned LSTM and BERT models in terms of classification accuracy and other relevant metrics.

## Resources
The project will require the following resources:
- Access to the [COVID-19 NLP Text Classification dataset](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv) available on Kaggle.
- Knowledge and techniques learned in class, including fine-tuning techniques for LSTM and BERT models, preprocessing techniques, and evaluation metrics.
- Python, along with relevant libraries such as TensorFlow or PyTorch, for implementation.

## Conclusion
This project proposal outlines a plan to fine-tune LSTM and BERT Transformer models for text classification using the COVID-19 NLP Text Classification dataset. By applying fine-tuning techniques, we aim to improve the performance of these models specifically for the COVID-19 text classification task. The project will provide valuable insights into the effectiveness of fine-tuning for enhancing the classification accuracy and performance
